{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets==2.15.0\n!pip install evaluate","metadata":{"execution":{"iopub.status.busy":"2024-03-22T20:51:36.608819Z","iopub.execute_input":"2024-03-22T20:51:36.609160Z","iopub.status.idle":"2024-03-22T20:51:54.163623Z","shell.execute_reply.started":"2024-03-22T20:51:36.609128Z","shell.execute_reply":"2024-03-22T20:51:54.162469Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting datasets==2.15.0\n  Downloading datasets-2.15.0-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (1.26.4)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (11.0.0)\nCollecting pyarrow-hotfix (from datasets==2.15.0)\n  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\nCollecting dill<0.3.8,>=0.3.0 (from datasets==2.15.0)\n  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (0.70.16)\nCollecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.15.0)\n  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.18.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (0.21.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (4.0.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.18.0->datasets==2.15.0) (3.13.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.18.0->datasets==2.15.0) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets==2.15.0) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.15.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.15.0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.15.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.15.0) (2024.2.2)\nINFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\nCollecting multiprocess (from datasets==2.15.0)\n  Downloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.15.0) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.15.0) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.15.0) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.15.0) (1.16.0)\nDownloading datasets-2.15.0-py3-none-any.whl (521 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading dill-0.3.7-py3-none-any.whl (115 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\nInstalling collected packages: pyarrow-hotfix, fsspec, dill, multiprocess, datasets\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2024.3.0\n    Uninstalling fsspec-2024.3.0:\n      Successfully uninstalled fsspec-2024.3.0\n  Attempting uninstall: dill\n    Found existing installation: dill 0.3.8\n    Uninstalling dill-0.3.8:\n      Successfully uninstalled dill-0.3.8\n  Attempting uninstall: multiprocess\n    Found existing installation: multiprocess 0.70.16\n    Uninstalling multiprocess-0.70.16:\n      Successfully uninstalled multiprocess-0.70.16\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.1.0\n    Uninstalling datasets-2.1.0:\n      Successfully uninstalled datasets-2.1.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.4.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\ngcsfs 2023.12.2.post1 requires fsspec==2023.12.2, but you have fsspec 2023.10.0 which is incompatible.\npathos 0.3.2 requires dill>=0.3.8, but you have dill 0.3.7 which is incompatible.\npathos 0.3.2 requires multiprocess>=0.70.16, but you have multiprocess 0.70.15 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\ns3fs 2024.3.0 requires fsspec==2024.3.0, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed datasets-2.15.0 dill-0.3.7 fsspec-2023.10.0 multiprocess-0.70.15 pyarrow-hotfix-0.6\n","output_type":"stream"}]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-22T20:52:17.732534Z","iopub.execute_input":"2024-03-22T20:52:17.732927Z","iopub.status.idle":"2024-03-22T20:52:19.598927Z","shell.execute_reply.started":"2024-03-22T20:52:17.732887Z","shell.execute_reply":"2024-03-22T20:52:19.598161Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Loading the dataset\ngtzan = load_dataset(\"marsyas/gtzan\", \"all\")\ngtzan","metadata":{"execution":{"iopub.status.busy":"2024-03-22T20:52:19.600445Z","iopub.execute_input":"2024-03-22T20:52:19.600827Z","iopub.status.idle":"2024-03-22T20:53:00.012831Z","shell.execute_reply.started":"2024-03-22T20:52:19.600801Z","shell.execute_reply":"2024-03-22T20:53:00.011891Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/3.35k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eace0ebc00c24bd3a1f0fc9988742274"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/4.42k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d5d5ca73bfe4087a10bda95a48975c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.23G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad3020f2e1b8475aa65075c91d68bd2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7375fac73ae465fa8d34ff83b0514cd"}},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['file', 'audio', 'genre'],\n        num_rows: 999\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"# Splitting the data into training and testing set\ngtzan = gtzan[\"train\"].train_test_split(seed=42, shuffle=True,\n                                        test_size=0.2)\ngtzan","metadata":{"execution":{"iopub.status.busy":"2024-03-22T20:53:00.013866Z","iopub.execute_input":"2024-03-22T20:53:00.014140Z","iopub.status.idle":"2024-03-22T20:53:04.513251Z","shell.execute_reply.started":"2024-03-22T20:53:00.014115Z","shell.execute_reply":"2024-03-22T20:53:04.512189Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['file', 'audio', 'genre'],\n        num_rows: 799\n    })\n    test: Dataset({\n        features: ['file', 'audio', 'genre'],\n        num_rows: 200\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"# Looking at a sample\ngtzan[\"train\"][0]","metadata":{"execution":{"iopub.status.busy":"2024-03-22T20:53:04.516057Z","iopub.execute_input":"2024-03-22T20:53:04.516510Z","iopub.status.idle":"2024-03-22T20:53:14.767773Z","shell.execute_reply.started":"2024-03-22T20:53:04.516468Z","shell.execute_reply":"2024-03-22T20:53:14.766823Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"{'file': '/root/.cache/huggingface/datasets/downloads/extracted/5022b0984afa7334ff9a3c60566280b08b5179d4ac96a628052bada7d8940244/genres/rock/rock.00012.wav',\n 'audio': {'path': '/root/.cache/huggingface/datasets/downloads/extracted/5022b0984afa7334ff9a3c60566280b08b5179d4ac96a628052bada7d8940244/genres/rock/rock.00012.wav',\n  'array': array([-0.006073  ,  0.0144043 ,  0.04669189, ..., -0.00701904,\n         -0.01977539, -0.02853394]),\n  'sampling_rate': 22050},\n 'genre': 9}"},"metadata":{}}]},{"cell_type":"code","source":"# Converting the label to its string form\ngtzan[\"train\"].features[\"genre\"].int2str(9)","metadata":{"execution":{"iopub.status.busy":"2024-03-22T20:53:14.769218Z","iopub.execute_input":"2024-03-22T20:53:14.770167Z","iopub.status.idle":"2024-03-22T20:53:14.776736Z","shell.execute_reply.started":"2024-03-22T20:53:14.770126Z","shell.execute_reply":"2024-03-22T20:53:14.775889Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"'rock'"},"metadata":{}}]},{"cell_type":"code","source":"# Loading HuBERT\nfrom transformers import AutoFeatureExtractor\n\nMODEL = \"ntu-spml/distilhubert\"\n\nfeatures = AutoFeatureExtractor.from_pretrained(\n    MODEL, do_normalize=True, return_attention_mask=True\n)\n\nfeatures.sampling_rate","metadata":{"execution":{"iopub.status.busy":"2024-03-22T20:53:14.777761Z","iopub.execute_input":"2024-03-22T20:53:14.778093Z","iopub.status.idle":"2024-03-22T20:53:24.945579Z","shell.execute_reply.started":"2024-03-22T20:53:14.778067Z","shell.execute_reply":"2024-03-22T20:53:24.944666Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/214 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2456bb67282b43308abd867c02ea40d3"}},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"16000"},"metadata":{}}]},{"cell_type":"markdown","source":"Since the sampling rate of the model and training samples is different, we have to resample the dataset.","metadata":{}},{"cell_type":"code","source":"from datasets import Audio\n\ngtzan = gtzan.cast_column(\"audio\", \n                          Audio(sampling_rate=features.sampling_rate))","metadata":{"execution":{"iopub.status.busy":"2024-03-22T20:53:24.946937Z","iopub.execute_input":"2024-03-22T20:53:24.947530Z","iopub.status.idle":"2024-03-22T20:53:24.961220Z","shell.execute_reply.started":"2024-03-22T20:53:24.947496Z","shell.execute_reply":"2024-03-22T20:53:24.960373Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Checking to see if that worked,","metadata":{}},{"cell_type":"code","source":"gtzan[\"train\"][0]","metadata":{"execution":{"iopub.status.busy":"2024-03-22T20:53:24.962207Z","iopub.execute_input":"2024-03-22T20:53:24.962569Z","iopub.status.idle":"2024-03-22T20:53:25.006127Z","shell.execute_reply.started":"2024-03-22T20:53:24.962539Z","shell.execute_reply":"2024-03-22T20:53:25.005281Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"{'file': '/root/.cache/huggingface/datasets/downloads/extracted/5022b0984afa7334ff9a3c60566280b08b5179d4ac96a628052bada7d8940244/genres/rock/rock.00012.wav',\n 'audio': {'path': '/root/.cache/huggingface/datasets/downloads/extracted/5022b0984afa7334ff9a3c60566280b08b5179d4ac96a628052bada7d8940244/genres/rock/rock.00012.wav',\n  'array': array([-0.00319324,  0.0242455 ,  0.06524472, ..., -0.00888791,\n         -0.02288128,  0.        ]),\n  'sampling_rate': 16000},\n 'genre': 9}"},"metadata":{}}]},{"cell_type":"code","source":"def preprocess_function(examples):\n    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n    inputs = features(\n        audio_arrays,\n        sampling_rate=features.sampling_rate,\n        max_length=int(features.sampling_rate * 30.0),\n        truncation=True,\n        return_attention_mask=True,\n    )\n    return inputs\n\ngtzan_encoded = gtzan.map(\n    preprocess_function,\n    remove_columns=[\"audio\", \"file\"],\n    batched=True,\n    batch_size=100,\n    num_proc=1,\n)\ngtzan_encoded","metadata":{"execution":{"iopub.status.busy":"2024-03-22T20:53:25.007186Z","iopub.execute_input":"2024-03-22T20:53:25.007448Z","iopub.status.idle":"2024-03-22T20:54:20.430406Z","shell.execute_reply.started":"2024-03-22T20:53:25.007425Z","shell.execute_reply":"2024-03-22T20:54:20.429473Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/799 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d342468b0db4914a45f3d4c0ff587b7"}},"metadata":{}},{"name":"stderr","text":"2024-03-22 20:53:30.933879: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-22 20:53:30.933979: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-22 20:53:31.341456: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34586c66396f4bc1a6d7aebe715c19d1"}},"metadata":{}},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['genre', 'input_values', 'attention_mask'],\n        num_rows: 799\n    })\n    test: Dataset({\n        features: ['genre', 'input_values', 'attention_mask'],\n        num_rows: 200\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"# Changing the column name\ngtzan_encoded = gtzan_encoded.rename_column(\"genre\", \"label\")","metadata":{"execution":{"iopub.status.busy":"2024-03-22T20:54:20.434235Z","iopub.execute_input":"2024-03-22T20:54:20.435100Z","iopub.status.idle":"2024-03-22T20:54:20.443494Z","shell.execute_reply.started":"2024-03-22T20:54:20.435064Z","shell.execute_reply":"2024-03-22T20:54:20.442683Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"gtzan_encoded","metadata":{"execution":{"iopub.status.busy":"2024-03-22T20:54:20.444770Z","iopub.execute_input":"2024-03-22T20:54:20.445115Z","iopub.status.idle":"2024-03-22T20:54:23.435328Z","shell.execute_reply.started":"2024-03-22T20:54:20.445085Z","shell.execute_reply":"2024-03-22T20:54:23.434350Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['label', 'input_values', 'attention_mask'],\n        num_rows: 799\n    })\n    test: Dataset({\n        features: ['label', 'input_values', 'attention_mask'],\n        num_rows: 200\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"id2label_fn = gtzan[\"train\"].features[\"genre\"].int2str\n\nid2label = {\n    str(i): id2label_fn(i) for i in range(len(gtzan_encoded[\"train\"].features[\"label\"].names))\n}\nlabel2id = {v: k for k, v in id2label.items()}\n\nid2label[\"9\"]","metadata":{"execution":{"iopub.status.busy":"2024-03-22T20:54:23.436562Z","iopub.execute_input":"2024-03-22T20:54:23.436917Z","iopub.status.idle":"2024-03-22T20:54:23.449824Z","shell.execute_reply.started":"2024-03-22T20:54:23.436881Z","shell.execute_reply":"2024-03-22T20:54:23.448966Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"'rock'"},"metadata":{}}]},{"cell_type":"markdown","source":"### Fine-tuning the model","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForAudioClassification\n\nmodel = AutoModelForAudioClassification.from_pretrained(\n    MODEL,\n    num_labels=len(id2label),\n    label2id=label2id,\n    id2label=id2label\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-22T20:54:23.451555Z","iopub.execute_input":"2024-03-22T20:54:23.452164Z","iopub.status.idle":"2024-03-22T20:54:26.128087Z","shell.execute_reply.started":"2024-03-22T20:54:23.452129Z","shell.execute_reply":"2024-03-22T20:54:26.127160Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.30k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b125bf6c3764d98abf9fd1d471df779"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/94.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"408d3813f91948e39daaa0b7e96a2af3"}},"metadata":{}},{"name":"stderr","text":"Some weights of HubertForSequenceClassification were not initialized from the model checkpoint at ntu-spml/distilhubert and are newly initialized: ['classifier.bias', 'classifier.weight', 'encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'projector.bias', 'projector.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-03-22T21:00:56.435267Z","iopub.execute_input":"2024-03-22T21:00:56.435644Z","iopub.status.idle":"2024-03-22T21:00:56.457233Z","shell.execute_reply.started":"2024-03-22T21:00:56.435615Z","shell.execute_reply":"2024-03-22T21:00:56.456321Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acdf661545ae4842a13137a2956e06b2"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    \"distilhubert-GTZAN\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=5e-5,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=1,\n    per_device_eval_batch_size=4,\n    num_train_epochs=10,\n    warmup_ratio=0.1,\n    logging_steps=5,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    fp16=True,\n    push_to_hub=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-22T21:11:32.915879Z","iopub.execute_input":"2024-03-22T21:11:32.916590Z","iopub.status.idle":"2024-03-22T21:11:32.927008Z","shell.execute_reply.started":"2024-03-22T21:11:32.916552Z","shell.execute_reply":"2024-03-22T21:11:32.925763Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# Defining the metrics\nimport evaluate\nimport numpy as np\n\nmetric = evaluate.load(\"accuracy\")\n\n\ndef compute_metrics(eval_pred):\n    predictions = np.argmax(eval_pred.predictions, axis=1)\n    return metric.compute(predictions=predictions, references=eval_pred.label_ids)","metadata":{"execution":{"iopub.status.busy":"2024-03-22T21:11:32.946124Z","iopub.execute_input":"2024-03-22T21:11:32.946918Z","iopub.status.idle":"2024-03-22T21:11:33.328396Z","shell.execute_reply.started":"2024-03-22T21:11:32.946879Z","shell.execute_reply":"2024-03-22T21:11:33.327406Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# Training the model\nfrom transformers import Trainer\n\ntrainer = Trainer(\n    model,\n    training_args,\n    train_dataset=gtzan_encoded[\"train\"],\n    eval_dataset=gtzan_encoded[\"test\"],\n    tokenizer=features,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-03-22T21:11:33.497906Z","iopub.execute_input":"2024-03-22T21:11:33.498183Z","iopub.status.idle":"2024-03-22T23:23:58.407931Z","shell.execute_reply.started":"2024-03-22T21:11:33.498158Z","shell.execute_reply":"2024-03-22T23:23:58.406769Z"},"trusted":true},"execution_count":43,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1000/1000 2:12:08, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.898800</td>\n      <td>1.873726</td>\n      <td>0.565000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.356300</td>\n      <td>1.371171</td>\n      <td>0.665000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.075400</td>\n      <td>1.100484</td>\n      <td>0.705000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.617500</td>\n      <td>0.991849</td>\n      <td>0.695000</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.511800</td>\n      <td>0.926035</td>\n      <td>0.670000</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.413500</td>\n      <td>0.761106</td>\n      <td>0.760000</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.323900</td>\n      <td>0.802893</td>\n      <td>0.755000</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.276400</td>\n      <td>0.726452</td>\n      <td>0.780000</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.142300</td>\n      <td>0.707583</td>\n      <td>0.775000</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.120100</td>\n      <td>0.726943</td>\n      <td>0.775000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1000, training_loss=0.7807679140567779, metrics={'train_runtime': 7940.6927, 'train_samples_per_second': 1.006, 'train_steps_per_second': 0.126, 'total_flos': 5.451675897024e+17, 'train_loss': 0.7807679140567779, 'epoch': 10.0})"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}